{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rap Text Generator\n",
    "\n",
    "Student Name: Dmitry Timerbaev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import lyricsgenius\n",
    "from nltk.lm import MLE\n",
    "from nltk.util import ngrams\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the tokenizer\n",
    "try: # Use the default NLTK tokenizer.\n",
    "    from nltk import word_tokenize, sent_tokenize \n",
    "    word_tokenize(sent_tokenize(\"This is a foobar sentence. Yes it is.\")[0])\n",
    "except: # Use a naive sentence tokenizer and toktok.\n",
    "    import re\n",
    "    from nltk.tokenize import ToktokTokenizer\n",
    "    sent_tokenize = lambda x: re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', x)\n",
    "    toktok = ToktokTokenizer()\n",
    "    word_tokenize = word_tokenize = toktok.tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data retrieval\n",
    "LyricsGenius API was used to retrieve lyrics of 10 most popular Eminem songs. For creating rap generator, I tried to use n-gram model and recurrent neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up API token\n",
    "genius = lyricsgenius.Genius(\"rPXL2JkaA1EqVzgpJxFGFocg149ZOiUveQWeaNTsMo51Dq125_dNzjfISkivKzFr\")\n",
    "genius.remove_section_headers = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"Rap God\" by Eminem...\n",
      "Done.\n",
      "Searching for \"Killshot\" by Eminem...\n",
      "Done.\n",
      "Searching for \"Lose Yourself\" by Eminem...\n",
      "Done.\n",
      "Searching for \"The Monster\" by Eminem...\n",
      "Done.\n",
      "Searching for \"Lucky You\" by Eminem...\n",
      "Done.\n",
      "Searching for \"Godzilla\" by Eminem...\n",
      "Done.\n",
      "Searching for \"The Ringer\" by Eminem...\n",
      "Done.\n",
      "Searching for \"River\" by Eminem...\n",
      "Done.\n",
      "Searching for \"Berzerk\" by Eminem...\n",
      "Done.\n",
      "Searching for \"Venom\" by Eminem...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# download train lyrics (10 most popular Eminem songs)\n",
    "train_song_list = ['Rap God', 'Killshot', 'Lose Yourself', 'The Monster', 'Lucky You', 'Godzilla', 'The Ringer', 'River', 'Berzerk', 'Venom']\n",
    "data_1 = \"\"\n",
    "for i in train_song_list:\n",
    "    song = genius.search_song(i, 'Eminem')\n",
    "    data_1 += song.lyrics + \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_data = data_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "I splitted data into sentences by line breaks. Then, I divided sentences into words, cleaned them of certain punctuation characters and reassembled again for further tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train data into sentences\n",
    "tr = re.split(r'\\n', train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function that removes any punctuation from strings\n",
    "def punctuation(string): \n",
    "  \n",
    "    # punctuation marks to be removed\n",
    "    punctuations = '''!;:?()\"—[]<>'''\n",
    "  \n",
    "    # goes through each character in a string and if character belongs to punctuation - makes it null \n",
    "    for x in string: \n",
    "        if x in punctuations: \n",
    "            string = string.replace(x, \"\") \n",
    "  \n",
    "    # returns the string in lowercase letters \n",
    "    return string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation from sentence elements\n",
    "words_list = []\n",
    "for i in range(len(tr)):\n",
    "    temporary_dict = []\n",
    "    for t in tr[i].split():\n",
    "        stg = punctuation(t)\n",
    "        temporary_dict.append(stg)\n",
    "    words_list.append(temporary_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['look',\n",
       " ',',\n",
       " 'i',\n",
       " 'was',\n",
       " 'gonna',\n",
       " 'go',\n",
       " 'easy',\n",
       " 'on',\n",
       " 'you',\n",
       " 'not',\n",
       " 'to',\n",
       " 'hurt',\n",
       " 'your',\n",
       " 'feelings',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recreate sentences and then tokenize by word. check the tokenized sentence\n",
    "sent_list = [x for x in words_list if x != []]\n",
    "new_sent_list = [' '.join(sent) for sent in sent_list]\n",
    "tokenized = [list(map(str.lower, word_tokenize(sent))) for sent in new_sent_list]\n",
    "tokenized[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model and generate sample rap\n",
    "I used 3-gram MLE model to fit the tokenized dataset, and create the text generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the tokenized text for 3-gram language modelling\n",
    "n = 3 \n",
    "train_d, padded_sents = padded_everygram_pipeline(n, tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2323"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model, check the length of vocabulary\n",
    "model = MLE(n)\n",
    "model.fit(train_d, padded_sents)\n",
    "len(model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'villain', 'outta', 'those', 'blockbusters', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# generate a single sentence\n",
    "print(model.generate(30, random_seed=25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating function that converts generated text into readable form\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "detokenize = TreebankWordDetokenizer().detokenize\n",
    "\n",
    "def generate_sent(model, num_words, random_seed=42):\n",
    "    content = []\n",
    "    for token in model.generate(num_words, random_seed=random_seed):\n",
    "        if token == '<s>':\n",
    "            continue\n",
    "        if token == '</s>':\n",
    "            break\n",
    "        content.append(token)\n",
    "    return detokenize(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a villain outta those blockbusters'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate same single sentence in new form\n",
    "generate_sent(model, 30, random_seed=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "straight out the coupe, hop out and booed off stage\n",
      "when you' ll take you back\n",
      "in the right type of life for my music\n",
      "record every time i break a motherfuckin' optionfailure' s the only opportunity that i' m reloadin '\n",
      "pull my mac out and shoot\n",
      "but i' m' bout to bloody this track up, overblaow\n",
      "head\n",
      "bitch\n",
      "caught slippin '\n",
      "i get on a guy with a pipe wrench\n",
      ".\n",
      "with\n",
      "i' m not done preach\n",
      "fame made me a costly mistake\n",
      "ll still be like everyone else in the front smashed, much as my rear fender, assassin\n",
      "time to go i cannot grow old in salem' s your moment\n",
      "when i' ve been a lover, been a thief\n",
      "tough times\n",
      "midst of all this\n",
      "a villain outta those blockbusters\n",
      "of defeat and rise to my feet\n",
      "just pulled a pistol on a mic\n",
      "give me the juice\n"
     ]
    }
   ],
   "source": [
    "# sample rap generation with 30 lines\n",
    "for i in range(30):\n",
    "    if generate_sent(model,30,random_seed=i) == '':\n",
    "        continue\n",
    "    else:\n",
    "        print(generate_sent(model,30,random_seed=i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "Before training RNN, string data needs to be mapped to a numerical representation. Next data is to be divided into example sequences. For each input sequence, the corresponding targets contain the same length of text, except shifted one character to the right. Finally, training batches need to be created - those batches will be fed to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "text = data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87 unique characters\n"
     ]
    }
   ],
   "source": [
    "# set up vocabulary and investigate unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\n",
      "L\n",
      "o\n",
      "o\n",
      "k\n"
     ]
    }
   ],
   "source": [
    "# set up maximum length sentence we want for a single input in characters\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "# create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "    print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\"Look, I was gonna go easy on you not to hurt your feelings.\"\\n\"But I\\'m only going to get this one cha'\n",
      "'nce.\" (Six minutes— Six minutes—)\\n\"Something\\'s wrong, I can feel it.\" (Six minutes, Slim Shady, you\\'r'\n",
      "'e on!)\\n\"Just a feeling I\\'ve got. Like something\\'s about to happen, but I don\\'t know what.\\xa0\\nIf that me'\n",
      "\"ans what I think it means, we're in trouble, big trouble;\\xa0\\nAnd if he is as bananas as you say, I'm no\"\n",
      "'t taking any chances.\"\\n\"You are just what the doc ordered.\"\\n\\nI\\'m beginnin\\' to feel like a Rap God, Ra'\n"
     ]
    }
   ],
   "source": [
    "# try out conversion of characters into sequences of desired size. looks fine.\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "    print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  '\"Look, I was gonna go easy on you not to hurt your feelings.\"\\n\"But I\\'m only going to get this one ch'\n",
      "Target data: 'Look, I was gonna go easy on you not to hurt your feelings.\"\\n\"But I\\'m only going to get this one cha'\n"
     ]
    }
   ],
   "source": [
    "# for each sequence, duplicate and shift it to form the input and target text by using the map method to apply a simple function to each batch\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n",
    "\n",
    "# try printing the first examples input and target values\n",
    "for input_example, target_example in  dataset.take(1):\n",
    "    print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create training batches \n",
    "\n",
    "# batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "# set up the dataset for training\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model\n",
    "Building the RNN for text generation requires certain steps:<br>\n",
    "1) Define model architecture - I used simple RNN structure with 1 embedding input layer, GRU (LSTM could also be used) and dense as output layer<br>\n",
    "2) Choose optimizer and loss function - I compiled model with Adam optimizer and categorical cross-entropy loss<br>\n",
    "3) Configure checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "\n",
    "# length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# number of RNN units\n",
    "rnn_units = 1024\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the training model\n",
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (64, None, 256)           22272     \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (64, None, 87)            89175     \n",
      "=================================================================\n",
      "Total params: 4,049,751\n",
      "Trainable params: 4,049,751\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# show model summary - 1 embedding, gru and dense layers\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 87)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.467277\n"
     ]
    }
   ],
   "source": [
    "# define loss function (sparse categorical cross-entropy). this loss function works because it is applied across the last dimension of the predictions\n",
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model. use adam optimizer and defined loss function\n",
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up checkpoints\n",
    "\n",
    "# directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model and generate sample rap\n",
    "I trained RNN on 100 epochs and used batch size of 1 to generate random rap lyrics (RNN state is passed from timestep to timestep, the model only accepts a fixed batch size once built)<br>\n",
    "\n",
    "Text generation is achieved through prediction loop:<br>\n",
    "- It Starts by choosing a start string, initializing the RNN state and setting the number of characters to generate.\n",
    "\n",
    "- Get the prediction distribution of the next character using the start string and the RNN state.\n",
    "\n",
    "- Then, use a categorical distribution to calculate the index of the predicted character. Use this predicted character as our next input to the model.\n",
    "\n",
    "- The RNN state returned by the model is fed back into the model so that it now has more context, instead than only one character. After predicting the next character, the modified RNN states are again fed back into the model, which is how it learns as it gets more context from the previously predicted characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
      "7/7 [==============================] - 43s 6s/step - loss: 4.8836\n",
      "Epoch 2/100\n",
      "7/7 [==============================] - 38s 5s/step - loss: 4.1876\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - 37s 5s/step - loss: 3.8614\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - 27s 4s/step - loss: 3.3205\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - 27s 4s/step - loss: 3.1191\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - 27s 4s/step - loss: 3.0127\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - 27s 4s/step - loss: 2.8935\n",
      "Epoch 8/100\n",
      "7/7 [==============================] - 25s 4s/step - loss: 2.7840\n",
      "Epoch 9/100\n",
      "7/7 [==============================] - 27s 4s/step - loss: 2.6791\n",
      "Epoch 10/100\n",
      "7/7 [==============================] - 29s 4s/step - loss: 2.6043\n",
      "Epoch 11/100\n",
      "7/7 [==============================] - 27s 4s/step - loss: 2.5351\n",
      "Epoch 12/100\n",
      "7/7 [==============================] - 26s 4s/step - loss: 2.4801\n",
      "Epoch 13/100\n",
      "7/7 [==============================] - 26s 4s/step - loss: 2.4408\n",
      "Epoch 14/100\n",
      "7/7 [==============================] - 26s 4s/step - loss: 2.4117\n",
      "Epoch 15/100\n",
      "7/7 [==============================] - 26s 4s/step - loss: 2.3874\n",
      "Epoch 16/100\n",
      "7/7 [==============================] - 27s 4s/step - loss: 2.3523\n",
      "Epoch 17/100\n",
      "7/7 [==============================] - 27s 4s/step - loss: 2.3299\n",
      "Epoch 18/100\n",
      "7/7 [==============================] - 30s 4s/step - loss: 2.3061\n",
      "Epoch 19/100\n",
      "7/7 [==============================] - 27s 4s/step - loss: 2.2815\n",
      "Epoch 20/100\n",
      "7/7 [==============================] - 29s 4s/step - loss: 2.2631\n",
      "Epoch 21/100\n",
      "7/7 [==============================] - 32s 5s/step - loss: 2.2426\n",
      "Epoch 22/100\n",
      "7/7 [==============================] - 27s 4s/step - loss: 2.2108\n",
      "Epoch 23/100\n",
      "7/7 [==============================] - 28s 4s/step - loss: 2.1964\n",
      "Epoch 24/100\n",
      "7/7 [==============================] - 28s 4s/step - loss: 2.1570\n",
      "Epoch 25/100\n",
      "7/7 [==============================] - 29s 4s/step - loss: 2.1230\n",
      "Epoch 26/100\n",
      "7/7 [==============================] - 26s 4s/step - loss: 2.1160\n",
      "Epoch 27/100\n",
      "7/7 [==============================] - 27s 4s/step - loss: 2.0963\n",
      "Epoch 28/100\n",
      "7/7 [==============================] - 27s 4s/step - loss: 2.0698\n",
      "Epoch 29/100\n",
      "7/7 [==============================] - 25s 4s/step - loss: 2.0404\n",
      "Epoch 30/100\n",
      "7/7 [==============================] - 26s 4s/step - loss: 2.0114\n",
      "Epoch 31/100\n",
      "7/7 [==============================] - 26s 4s/step - loss: 1.9926\n",
      "Epoch 32/100\n",
      "7/7 [==============================] - 26s 4s/step - loss: 1.9651\n",
      "Epoch 33/100\n",
      "7/7 [==============================] - 27s 4s/step - loss: 1.9429\n",
      "Epoch 34/100\n",
      "7/7 [==============================] - 26s 4s/step - loss: 1.9219\n",
      "Epoch 35/100\n",
      "7/7 [==============================] - 26s 4s/step - loss: 1.8835\n",
      "Epoch 36/100\n",
      "7/7 [==============================] - 26s 4s/step - loss: 1.8705\n",
      "Epoch 37/100\n",
      "7/7 [==============================] - 27s 4s/step - loss: 1.8461\n",
      "Epoch 38/100\n",
      "7/7 [==============================] - 26s 4s/step - loss: 1.8279\n",
      "Epoch 39/100\n",
      "7/7 [==============================] - 28s 4s/step - loss: 1.7933\n",
      "Epoch 40/100\n",
      "7/7 [==============================] - 29s 4s/step - loss: 1.7610\n",
      "Epoch 41/100\n",
      "7/7 [==============================] - 27s 4s/step - loss: 1.7327\n",
      "Epoch 42/100\n",
      "7/7 [==============================] - 26s 4s/step - loss: 1.7130\n",
      "Epoch 43/100\n",
      "7/7 [==============================] - 33s 5s/step - loss: 1.6742\n",
      "Epoch 44/100\n",
      "7/7 [==============================] - 27s 4s/step - loss: 1.6525\n",
      "Epoch 45/100\n",
      "7/7 [==============================] - 26s 4s/step - loss: 1.6199\n",
      "Epoch 46/100\n",
      "7/7 [==============================] - 26s 4s/step - loss: 1.5712\n",
      "Epoch 47/100\n",
      "7/7 [==============================] - 26s 4s/step - loss: 1.5537\n",
      "Epoch 48/100\n",
      "7/7 [==============================] - 26s 4s/step - loss: 1.5242\n",
      "Epoch 49/100\n",
      "7/7 [==============================] - 27s 4s/step - loss: 1.5041\n",
      "Epoch 50/100\n",
      "7/7 [==============================] - 27s 4s/step - loss: 1.4470\n",
      "Epoch 51/100\n",
      "7/7 [==============================] - 26s 4s/step - loss: 1.4159\n",
      "Epoch 52/100\n",
      "7/7 [==============================] - 28s 4s/step - loss: 1.3854\n",
      "Epoch 53/100\n",
      "7/7 [==============================] - 28s 4s/step - loss: 1.3453\n",
      "Epoch 54/100\n",
      "7/7 [==============================] - 38s 5s/step - loss: 1.2985\n",
      "Epoch 55/100\n",
      "7/7 [==============================] - 28s 4s/step - loss: 1.2694\n",
      "Epoch 56/100\n",
      "7/7 [==============================] - 27s 4s/step - loss: 1.2214\n",
      "Epoch 57/100\n",
      "7/7 [==============================] - 29s 4s/step - loss: 1.1916\n",
      "Epoch 58/100\n",
      "7/7 [==============================] - 30s 4s/step - loss: 1.1508\n",
      "Epoch 59/100\n",
      "7/7 [==============================] - 31s 4s/step - loss: 1.0964\n",
      "Epoch 60/100\n",
      "7/7 [==============================] - 28s 4s/step - loss: 1.0618\n",
      "Epoch 61/100\n",
      "7/7 [==============================] - 27s 4s/step - loss: 1.0131\n",
      "Epoch 62/100\n",
      "7/7 [==============================] - 28s 4s/step - loss: 0.9741\n",
      "Epoch 63/100\n",
      "7/7 [==============================] - 28s 4s/step - loss: 0.9279\n",
      "Epoch 64/100\n",
      "7/7 [==============================] - 27s 4s/step - loss: 0.8738\n",
      "Epoch 65/100\n",
      "7/7 [==============================] - 27s 4s/step - loss: 0.8397\n",
      "Epoch 66/100\n",
      "7/7 [==============================] - 27s 4s/step - loss: 0.7775\n",
      "Epoch 67/100\n",
      "7/7 [==============================] - 27s 4s/step - loss: 0.7393\n",
      "Epoch 68/100\n",
      "7/7 [==============================] - 27s 4s/step - loss: 0.7059\n",
      "Epoch 69/100\n",
      "7/7 [==============================] - 26s 4s/step - loss: 0.6607\n",
      "Epoch 70/100\n",
      "7/7 [==============================] - 27s 4s/step - loss: 0.6134\n",
      "Epoch 71/100\n",
      "7/7 [==============================] - 29s 4s/step - loss: 0.5744\n",
      "Epoch 72/100\n",
      "7/7 [==============================] - 27s 4s/step - loss: 0.5435\n",
      "Epoch 73/100\n",
      "7/7 [==============================] - 27s 4s/step - loss: 0.5128\n",
      "Epoch 74/100\n",
      "7/7 [==============================] - 28s 4s/step - loss: 0.4794\n",
      "Epoch 75/100\n",
      "7/7 [==============================] - 27s 4s/step - loss: 0.4432\n",
      "Epoch 76/100\n",
      "7/7 [==============================] - 27s 4s/step - loss: 0.4187\n",
      "Epoch 77/100\n",
      "7/7 [==============================] - 26s 4s/step - loss: 0.3967\n",
      "Epoch 78/100\n",
      "7/7 [==============================] - 27s 4s/step - loss: 0.3682\n",
      "Epoch 79/100\n",
      "7/7 [==============================] - 27s 4s/step - loss: 0.3515\n",
      "Epoch 80/100\n",
      "7/7 [==============================] - 27s 4s/step - loss: 0.3384\n",
      "Epoch 81/100\n",
      "7/7 [==============================] - 28s 4s/step - loss: 0.3174\n",
      "Epoch 82/100\n",
      "7/7 [==============================] - 26s 4s/step - loss: 0.3028\n",
      "Epoch 83/100\n",
      "7/7 [==============================] - 27s 4s/step - loss: 0.2885\n",
      "Epoch 84/100\n",
      "7/7 [==============================] - 27s 4s/step - loss: 0.2755\n",
      "Epoch 85/100\n",
      "7/7 [==============================] - 27s 4s/step - loss: 0.2659\n",
      "Epoch 86/100\n",
      "7/7 [==============================] - 27s 4s/step - loss: 0.2584\n",
      "Epoch 87/100\n",
      "7/7 [==============================] - 28s 4s/step - loss: 0.2477\n",
      "Epoch 88/100\n",
      "7/7 [==============================] - 28s 4s/step - loss: 0.2437\n",
      "Epoch 89/100\n",
      "7/7 [==============================] - 27s 4s/step - loss: 0.2320\n",
      "Epoch 90/100\n",
      "7/7 [==============================] - 28s 4s/step - loss: 0.2292\n",
      "Epoch 91/100\n",
      "7/7 [==============================] - 27s 4s/step - loss: 0.2240\n",
      "Epoch 92/100\n",
      "7/7 [==============================] - 27s 4s/step - loss: 0.2168\n",
      "Epoch 93/100\n",
      "7/7 [==============================] - 27s 4s/step - loss: 0.2138\n",
      "Epoch 94/100\n",
      "7/7 [==============================] - 27s 4s/step - loss: 0.2133\n",
      "Epoch 95/100\n",
      "7/7 [==============================] - 27s 4s/step - loss: 0.2069\n",
      "Epoch 96/100\n",
      "7/7 [==============================] - 27s 4s/step - loss: 0.2006\n",
      "Epoch 97/100\n",
      "7/7 [==============================] - 27s 4s/step - loss: 0.2032\n",
      "Epoch 98/100\n",
      "7/7 [==============================] - 27s 4s/step - loss: 0.1904\n",
      "Epoch 99/100\n",
      "7/7 [==============================] - 27s 4s/step - loss: 0.1970\n",
      "Epoch 100/100\n",
      "7/7 [==============================] - 26s 4s/step - loss: 0.1878\n"
     ]
    }
   ],
   "source": [
    "# fit the model with 100 epochs\n",
    "EPOCHS=100\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore last checkpoint; keep batch size of 1\n",
    "tf.train.latest_checkpoint(checkpoint_dir)\n",
    "\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function that generates text from given RNN model\n",
    "def generate_text(model, start_string):\n",
    "\n",
    "  # Number of characters to generate\n",
    "    num_generate = 500\n",
    "\n",
    "  # converting start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # empty string to store results\n",
    "    text_generated = []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # For this task, I will use low temperature text as default.\n",
    "    temperature = 1.0\n",
    "\n",
    "  # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the character returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # pass the predicted character as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Rap: chick, so I guess it ain't that waruble this out at the undergod and eain' so the scrobach the preath\n",
      "And you don't fuck with no Oath, fuck it\n",
      "What's your talknate get out the only think it's kill on the fenom\n",
      "Dnd for thater yout this that's what it's cold and shoot\n",
      "Aff all now and rick and I rap on his stamplline on the chainsaw\n",
      "'Cause Fab sait is hire\n",
      "I'ma even dast of blim\n",
      "Havit that I'd rather do than hear you on a mic\n",
      "Sin to piend, ye, Still I don't have any manners\n",
      "You got a couple of mans\n"
     ]
    }
   ],
   "source": [
    "# sample rap generation\n",
    "print(generate_text(model, start_string=u\"Sample Rap: \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text generated by both models somewhat resembles actual lyrics, but still TOO far from perfection. I would choose RNN model for text generation tasks, because it seems to have much more potential relative to n-gram.<br>**\n",
    "\n",
    "Suggestions for improvement:<br>\n",
    "\n",
    "1) Calculate perplexities and compare models (I was not able to figure out how to do that unfortunately)<br>\n",
    "2) Apply various smoothing techniques (Couldn't figure it out as well)<br>\n",
    "3) Build more complex RNN arhitecture (I could not do that due to limited computational power of my laptop)<br>\n",
    "4) Get more training data<br>\n",
    "\n",
    "_______________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References:\n",
    "- https://www.kaggle.com/alvations/n-gram-language-model-with-nltk (NLTK-LM tutorial)\n",
    "- https://www.tensorflow.org/tutorials/text/text_generation (TF RNN Text Generator tutorial)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
